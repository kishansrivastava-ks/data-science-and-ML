{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1  style = \"font-size:60px;background-color: Blue ; color : Orange; text-align: center; border-radius: 100px 100px;\">LOGISTIC REGRESSION</h1>","metadata":{}},{"cell_type":"markdown","source":"![Logistic Regression](https://miro.medium.com/max/1400/1*44qV8LhNzE5hPnta2PaaHw.png)\n\n**A classifier is an estimator that assigns a class label to the input data point. Logistic Regression is a linear classifier that gives out the probability of a data point belonging to a particular class & the data point is assigned a class label based on that probability value. \nAlso try to print out each step as it will make you understand better on what is going on.**","metadata":{}},{"cell_type":"markdown","source":"<h3 style= \"font-family: cursive;font-size: 50px;background-color: #D3D3D3; ;text-align:center;color:#800080;border-radius: 200px 200px;\">Importing Libraries ðŸ“–  </h3>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:42:55.140704Z","iopub.execute_input":"2023-05-03T16:42:55.141171Z","iopub.status.idle":"2023-05-03T16:42:56.173335Z","shell.execute_reply.started":"2023-05-03T16:42:55.141138Z","shell.execute_reply":"2023-05-03T16:42:56.172467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style= \"font-family: cursive;font-size: 50px;background-color: #D3D3D3; ;text-align:center;color:#800080;border-radius: 200px 200px;\">CREATE DATASET ðŸ“ˆ </h3>","metadata":{}},{"cell_type":"code","source":"# 1. Create Dataset\nX, y = make_blobs(n_samples = 1000, n_features = 2, centers=2, random_state=0) # basically you can make your regression/classification dataset through such methods. It is one of them \ndataset_array = np.concatenate((X, y.reshape(-1,1)), axis=1)\n\n# 2. Create a Dataframe of the array\ndataset_df = pd.DataFrame(dataset_array, columns = ['Col 1', 'Col 2', 'Target'])","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:42:56.175418Z","iopub.execute_input":"2023-05-03T16:42:56.175765Z","iopub.status.idle":"2023-05-03T16:42:56.191141Z","shell.execute_reply.started":"2023-05-03T16:42:56.175734Z","shell.execute_reply":"2023-05-03T16:42:56.189895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data=dataset_df, x='Col 1', y='Col 2', hue='Target')\nplt.xlabel(\"Column 1\")\nplt.ylabel(\"Column 2\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:42:56.192499Z","iopub.execute_input":"2023-05-03T16:42:56.192862Z","iopub.status.idle":"2023-05-03T16:42:56.603627Z","shell.execute_reply.started":"2023-05-03T16:42:56.192831Z","shell.execute_reply":"2023-05-03T16:42:56.602532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style= \"font-family: cursive;font-size: 50px;background-color: #D3D3D3; ;text-align:center;color:#800080;border-radius: 200px 200px;\">FUNCTIONS FOR LOGISTIC REGRESSION </h3>","metadata":{}},{"cell_type":"code","source":"# Build a logistic regression classifier\n\n# 0. Helper function: Sigmoid\ndef sigmoid(x):\n    \n    '''\n    sigmoid(x) = 1 / (1 + e^(-x))\n    '''\n    return 1 / (1 + np.exp(-x))\n\n# 1. Hypothesis\ndef hypothesis(x, theta):\n    \n    # h(x) = sigmoid(X.theta)\n    z = np.dot(X, theta)\n    \n    return sigmoid(z)\n\n# 2. Loss Function: Binary Cross Entropy\n# to get better idea try to print all of these by adding print statement in the function itself\ndef binary_cross_entropy(x, y, theta): # Usually we consider binary_cross_entropy as loss function(NOT ALWAYS) for binary classification\n    \n    m, n = x.shape \n    # print(x.shape)\n    # a. Compute the hypothesis\n    y_hat = hypothesis(x, theta)\n    # print(y_hat)\n    # b. Compute the Binary Cross Entropy\n    loss = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n    # print(np.log(y_hat))\n    \n    # print(np.log(1 - y_hat))\n    # print( np.mean(loss) )\n    return - np.mean(loss) \n\n# 3. Compute the gradient\ndef gradient(x, y, theta):\n    \n    # Compute hypothesis\n    y_hat = hypothesis(x, theta)\n    \n    # Compute gradient\n    grad = np.dot( x.T, (y - y_hat))\n    \n    return - grad / x.shape[0] \n\n# 4. Gradient Descent\ndef gradient_descent(x, y, n_iter = 100, alpha = 0.1): # Try to mingle with n_iter and alpha to see what happens when you change them\n    \n    # a. Randomly initialise theta\n    m,n = x.shape\n    theta = np.zeros(shape = (n, ))\n    \n    # List to store the error\n    error = []\n    \n    # b. Perform the gradient descent\n    for i in range(n_iter):\n        '''\n        y_hat = hypothesis(x, theta)\n        print(y_hat, y_hat.shape)\n        '''\n        \n        # b.1. Compute the loss\n        loss = binary_cross_entropy(x, y, theta)\n        error.append(loss)\n        \n        # b.2. Compute Gradient\n        grad = gradient(x, y, theta)\n        \n        # b.3. Perform the update rule\n        theta = theta - alpha * grad\n        \n    return theta, error","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:42:56.606611Z","iopub.execute_input":"2023-05-03T16:42:56.607339Z","iopub.status.idle":"2023-05-03T16:42:56.62061Z","shell.execute_reply.started":"2023-05-03T16:42:56.607289Z","shell.execute_reply":"2023-05-03T16:42:56.619413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Add constant\ndataset_df_copy = dataset_df.copy() \ndataset_df_copy.insert(0, 'Constant', np.ones(shape=(dataset_df_copy.shape[0], 1)))","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:42:56.621867Z","iopub.execute_input":"2023-05-03T16:42:56.622578Z","iopub.status.idle":"2023-05-03T16:42:56.638725Z","shell.execute_reply.started":"2023-05-03T16:42:56.622546Z","shell.execute_reply":"2023-05-03T16:42:56.637756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dataset_df_copy.drop('Target', axis=1)\ny = dataset_df_copy['Target']\n\ntheta, error = gradient_descent(X, y, 10000)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:42:56.640522Z","iopub.execute_input":"2023-05-03T16:42:56.6415Z","iopub.status.idle":"2023-05-03T16:43:06.979823Z","shell.execute_reply.started":"2023-05-03T16:42:56.64146Z","shell.execute_reply":"2023-05-03T16:43:06.978485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(theta)\nprint('*************************')\nprint(error[-1]) # last error","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:43:06.981889Z","iopub.execute_input":"2023-05-03T16:43:06.982325Z","iopub.status.idle":"2023-05-03T16:43:06.98937Z","shell.execute_reply.started":"2023-05-03T16:43:06.982284Z","shell.execute_reply":"2023-05-03T16:43:06.988241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(error)\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Error\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:43:06.990826Z","iopub.execute_input":"2023-05-03T16:43:06.991445Z","iopub.status.idle":"2023-05-03T16:43:07.230651Z","shell.execute_reply.started":"2023-05-03T16:43:06.991409Z","shell.execute_reply":"2023-05-03T16:43:07.229132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style= \"font-family: cursive;font-size: 50px;background-color: #D3D3D3; ;text-align:center;color:#800080;border-radius: 200px 200px;\">PLOTTING DATASET </h3>","metadata":{}},{"cell_type":"code","source":"# plot the dataset along with the deicision boundary\n\n\n# Create Decision Boundary\nx2_max, x2_min = X['Col 2'].max(), X['Col 2'].min()\nx1_max, x1_min = X['Col 1'].max(), X['Col 1'].min()\n\nx_vals = np.array([-2, 5])\nslope = - theta[1] / theta[2]\nintercept = - theta[0] / theta[2]\ndecision_boundary = slope * x_vals + intercept\n\n# Plot the dataset with decision bounddart\nplt.figure(figsize=(12,8))\nsns.scatterplot(data=dataset_df, x='Col 1', y='Col 2', hue='Target')\nplt.plot(x_vals, decision_boundary, linestyle='--', color='black', label='Decision Boundary')\nplt.fill_between(x_vals, decision_boundary, x2_min-10, color='tab:orange', alpha=0.2)\nplt.fill_between(x_vals, decision_boundary, x2_max+10, color='tab:blue', alpha=0.2)\nplt.xlabel(\"Column 1\")\nplt.ylabel(\"Column 2\")\nplt.ylim(x2_min-1, x2_max)\nplt.xlim(x1_min, 5)\nplt.legend(loc='best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:43:07.232497Z","iopub.execute_input":"2023-05-03T16:43:07.232956Z","iopub.status.idle":"2023-05-03T16:43:07.628184Z","shell.execute_reply.started":"2023-05-03T16:43:07.232921Z","shell.execute_reply":"2023-05-03T16:43:07.627445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing the results with Scikit Learn's Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Import logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\n# Build the model\nlr = LogisticRegression()\nlr.fit(X.drop('Constant', axis=1), y)\n\n# Compute coeffecients\ntheta_sklearn = lr.coef_\nintercept_sklearn = lr.intercept_","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:43:07.630798Z","iopub.execute_input":"2023-05-03T16:43:07.631362Z","iopub.status.idle":"2023-05-03T16:43:07.693434Z","shell.execute_reply.started":"2023-05-03T16:43:07.63133Z","shell.execute_reply":"2023-05-03T16:43:07.692395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the Decision Boundary\n# plot the dataset along with the deicision boundary\n\n# Create Decision Boundary\nx2_max, x2_min = X['Col 2'].max(), X['Col 2'].min()\nx1_max, x1_min = X['Col 1'].max(), X['Col 1'].min()\n\nx_vals = np.array([-2, 5])\nslope = - theta_sklearn[0][0] / theta_sklearn[0][1]\nintercept = - intercept_sklearn / theta_sklearn[0][1]\ndecision_boundary = slope * x_vals + intercept\n\n# Plot the dataset with decision bounddart\nplt.figure(figsize=(12,8))\nsns.scatterplot(data=dataset_df, x='Col 1', y='Col 2', hue='Target')\nplt.plot(x_vals, decision_boundary, linestyle='--', color='black', label='Decision Boundary')\nplt.fill_between(x_vals, decision_boundary, x2_min-10, color='tab:orange', alpha=0.2)\nplt.fill_between(x_vals, decision_boundary, x2_max+10, color='tab:blue', alpha=0.2)\nplt.xlabel(\"Column 1\")\nplt.ylabel(\"Column 2\")\nplt.ylim(x2_min-1, x1_max+4)\nplt.xlim(x1_min, 5)\nplt.legend(loc='best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:43:07.694592Z","iopub.execute_input":"2023-05-03T16:43:07.694875Z","iopub.status.idle":"2023-05-03T16:43:08.086651Z","shell.execute_reply.started":"2023-05-03T16:43:07.694849Z","shell.execute_reply":"2023-05-03T16:43:08.085446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the Custom Logisit Regression's results\nprint(\"Weights of variable given out by custom Logistic Regression\")\nprint(\"Col 1: {}\".format(theta[1]))\nprint(\"Col 2: {}\".format(theta[2]))\nprint(\"Intercept : {}\".format(theta[0]))\nprint()\n\nprint(\"Weights of variable given out by Sklearn's Logistic Regression\")\nprint(\"Col 1: {}\".format(theta_sklearn[0][0]))\nprint(\"Col 2: {}\".format(theta_sklearn[0][1]))\nprint(\"Intercept : {}\".format(intercept_sklearn[0]))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:43:08.088069Z","iopub.execute_input":"2023-05-03T16:43:08.08847Z","iopub.status.idle":"2023-05-03T16:43:08.097776Z","shell.execute_reply.started":"2023-05-03T16:43:08.088435Z","shell.execute_reply":"2023-05-03T16:43:08.096142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute accuracy for both the models\n\n# 1. Custom Logistic Regression\npredictions_1 = np.round(hypothesis(X.drop('Constant', axis=1), theta))\nacc1 = np.sum(predictions_1 == y) / len(y) * 100\n\n# 2. Sklearn's Logistic Regression\npredictions_2 = lr.predict(X.drop('Constant', axis=1))\nacc2 = np.sum(predictions_2 == y) / len(y) * 100\n\nprint(\"Accuracy of custom Logistic Regression Classifier: {}%\".format(acc1))\nprint(\"Accuracy of sklearn's Logistic Regression Classifier: {}%\".format(acc2))","metadata":{"execution":{"iopub.status.busy":"2023-05-03T16:43:08.099579Z","iopub.execute_input":"2023-05-03T16:43:08.100073Z","iopub.status.idle":"2023-05-03T16:43:08.114058Z","shell.execute_reply.started":"2023-05-03T16:43:08.10004Z","shell.execute_reply":"2023-05-03T16:43:08.112523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Try changing learning rate and num of iterations. Try taking learning rate very high and then very low(Dont take it -ve :). Also Check Scikit learn library and get familiar with it..it will come very handy in the future.","metadata":{}}]}